# DropMicroFluidAgents (DMFAs) üß†üîéüåêüßë‚Äçüíª

This repository accompanies the manuscript:  
**DropMicroFluidAgents (DMFAs): Autonomous Droplet Microfluidic Research Framework Through Large Language Model Agents**  
*Dinh-Nguyen Nguyen, Raymond Kai-Yu Tong, Ngoc-Duy Dinh*  
Department of Biomedical Engineering, The Chinese University of Hong Kong

**DropMicroFluidAgents (DMFAs)** is a novel multi-agent framework that leverages large language model (LLM) agents to automate and optimize research workflows in droplet microfluidics. It integrates scientific reasoning, machine learning, and CAD design automation into an intelligent and user-friendly system.


‚ñ∂Ô∏è [DEMO DMFAgents](https://github.com/duydinhlab/DMFAgents/blob/main/DEMO/DEMO-DMFAs.mp4)

## üì¨ Contact

- **Prof Ngoc-Duy Dinh** (Corresponding Author)  
  Email: ngocduydinh@cuhk.edu.hk
  
  Address: BME office, Room 1120, 11/F, William M.W. Mong Engineering Building, or 
  Room 208, Ho Sin Hang Engineering Building (SHB), The Chinese University of Hong Kong, Shatin, N.T., Hong Kong


## üîç Overview

DMFAs includes two main agents:
- **Scientific Mentor**: Provides accurate, domain-specific responses using Retrieval-Augmented Generation (RAG) and multiple LLMs (LLAMA 3.1, MISTRAL, GEMMA2).
- **Automation Designer**: Automatically generates machine learning models and CAD scripts for droplet microfluidic devices.


## üìà Key Features

- **Accurate Q&A** using a custom microfluidics knowledge base
- **Performance Tuning** via prompt engineering, embedding models, chunking strategies, and decoding parameters
- **CAD Design Generation** with validated AutoLISP scripts for AutoCAD
- **Machine Learning Automation** for droplet rate prediction (R¬≤ = 0.96)
- **Interactive GUI** for end-user accessibility

## üìä Performance

| Model | QA Accuracy | ML Model R¬≤ | CAD Support |
|-------|-------------|-------------|-------------|
| LLAMA 3.1 + DMFA | 76.15% | 0.96 | ‚úÖ |
| GEMMA2 + DMFA | +34.47% improvement | ‚Äì | ‚úÖ |
| MISTRAL + DMFA | 72.00% | ‚Äì | ‚úÖ |

## üìÅ Repository Structure

```
DMFAgents/
‚îú‚îÄ‚îÄ data/               # Microfluidics knowledge base, Q&A dataset
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ mentor/         # Scientific Mentor logic
‚îÇ   ‚îú‚îÄ‚îÄ designer/       # Automation Designer logic (ML & CAD)
‚îÇ   ‚îî‚îÄ‚îÄ utils/          # Prompting, embedding, and evaluation tools
‚îú‚îÄ‚îÄ gui/                # Streamlit-based UI for users
‚îú‚îÄ‚îÄ models/             # Pretrained models and evaluation results
‚îî‚îÄ‚îÄ README.md
```

## üß™ Evaluation

Evaluation includes:
- Accuracy and F1-score comparison of LLM and agent-based systems
- Performance across prompt strategies, embedding models, chunk sizes, and decoding parameters (top-p, top-k, temperature)
- Human expert verification of Q&A and CAD outputs


# DropMicroFluidAgents üß†üîéüåêüßë‚Äçüíª Scientific Mentor

This project implements a Scientific Mentor using LangChain, ChromaDB, SentenceTransformer embeddings, and a local Large Language Model (LLM) via Ollama. It processes PDF documents, chunks them into vector embeddings, and uses a modular LangGraph flow to answer scientific questions, particularly for droplet-based microfluidics research.

---

## Quick Start (Download and Run)

You do **not** need to clone the entire repo! Simply download the ` ScientificMentor_GUI.py ` file from GitHub and run it directly.


# Features

Load and chunk PDF documents using LangChain.
Embed and store documents in a local Chroma vectorstore.
Use a local LLM (e.g., llama3.1) for answer generation, grading, and routing.
Modular LangGraph flow including:
‚Ä¢	Question routing (vectorstore vs. web search)
‚Ä¢	Document relevance grading
‚Ä¢	Hallucination grading
‚Ä¢	Answer utility grading
Web search fallback with Tavily API

### 1. Download the Script

- Click the `Raw` button on the GitHub file view for ` ScientificMentor_GUI py `  
- Save the file to your local machine

### 2. Install Required Dependencies
you can install the required packages manually:
pip install langchain chromadb sentence-transformers PyPDF2 tavily-python nest_asyncio python-dotenv 
Note:
Python ‚â• 3.9
Ollama installed and running locally (for models like llama3.1)
Tavily API key for web search

### 3. Create .env File (Optional)
If you want to customize environment variables, create a .env file in the same directory as ScientificMentor.py

LLAMA_CLOUD_API_KEY=your_llama_api_key_here

COHERE_API_KEY=your_cohere_api_key_here

TAVILY_API_KEY=your_tavily_api_key_here

USER_AGENT=myagent 

If you skip this, the default USER_AGENT environment variable will be set by the app automatically.

### 4. PDF Setup
You may edit this in the script:
pdf_folder_path = '/your/path/to/RAG_droplet_downloaded_papers'

chroma_db_path = '/your/path/to/DATABASE'

The script will:
‚Ä¢	Extract text from all PDFs

‚Ä¢	Split into retrievable chunks

‚Ä¢	Store them in a persistent Chroma vector database

### 5. Run the App
Run the Streamlit app from your terminal:
streamlit run ScientificMentor_GUI.py

This will launch a local web server and open the app in your default browser.
Troubleshooting

No module named X ‚Üí Install missing module via pip install X

Ollama not running ‚Üí Make sure ollama is running locally

Missing .env ‚Üí Ensure .env is loaded and includes all necessary keys



# DropMicroFluidAgents üß†üîéüåêüßë‚Äçüíª Automation Designer

A Design Automation web app powered by **LLM-agents** that automates code generation, review, improvement, and evaluation for AutoLisp and Python code.  This application leverages LangGraph, LangChain, and the ChatOllama local LLM model (`llama3.1`) to create an intelligent code design assistant workflow.

---

## Features

- Generate code snippets from natural language questions  
- Review generated code for potential bugs and adherence to best practices  
- Improve and optimize code based on reviewer feedback  
- Rate and compare original vs improved code snippets  
- Classify unresolved feedback for iterative improvement  
- Supports AutoLisp and Python code languages  
- Runs locally using a lightweight, local LLM (ChatOllama) for privacy and speed  
- Simple Streamlit interface for easy user interaction  

---

## Quick Start (Download and Run)

You do **not** need to clone the entire repo! Simply download the ` AutomationDesigner_GUI.py ` file from GitHub and run it directly.

### 1. Download the Script

- Click the `Raw` button on the GitHub file view for ` AutomationDesigner_GUI.py `
-   
- Save the file to your local machine

### 2. Install Required Dependencies
you can install the required packages manually:
pip install streamlit python-dotenv langgraph langchain-core langchain-community typing-extensions
Note:
‚Ä¢	You must have Python 3.8 or higher installed.

‚Ä¢	You need to have a local Ollama LLM model (llama3.1) installed and running. See the Ollama docs for setup instructions.

### 3. Create .env File (Optional)
If you want to customize environment variables, create a .env file in the same directory as DesignAutomation.py

LLAMA_CLOUD_API_KEY=your_llama_api_key_here

USER_AGENT =myagent

If you skip this, the default USER_AGENT environment variable will be set by the app automatically.
‚úÖ Make sure your .env file is saved and loaded before running the script.

### 4. Run the App
Run the Streamlit app from your terminal:
streamlit run AutomationDesigner_GUI.py

This will launch a local web server and open the app in your default browser.
How to Use

1.	Select the programming language (AutoLisp or Python) from the dropdown.
2.	Enter your code-related question or request in the text area.
3.	Click the Run button to start the automated pipeline.
4.	View the outputs:
   
o	Original generated code
o	Final improved code after iterative reviews and improvements
o	Review history detailing feedback and changes

Code Structure Overview
‚Ä¢	Streamlit UI: Simple user interface for input and output

‚Ä¢	Prompt Templates: Custom prompts tailored for AutoLisp and Python code tasks (generation, review, improvement, rating, classification)

‚Ä¢	LangGraph Workflow: Defines nodes and transitions between code generator, reviewer, improver, and result stages

‚Ä¢	Local LLM (ChatOllama): Powered by a local LLM for text generation and JSON parsing

‚Ä¢	Environment loading: Uses python-dotenv to load environment variables

Requirements

‚Ä¢	Python 3.8+
‚Ä¢	Streamlit
‚Ä¢	python-dotenv
‚Ä¢	LangGraph
‚Ä¢	LangChain Core
‚Ä¢	LangChain Community
‚Ä¢	Typing Extensions
‚Ä¢	Local Ollama LLM with llama3.1 model installed and running

Troubleshooting

‚Ä¢	"ModuleNotFoundError": Make sure all required packages are installed via pip.

‚Ä¢	LLM model errors: Ensure you have the llama3.1 model properly installed locally and Ollama daemon/service is running.

‚Ä¢	Streamlit UI issues: Clear browser cache or try running in an incognito/private window.

‚Ä¢	Environment variables not loaded: Confirm your .env file is in the same directory or manually set variables in your OS environment.


## üîó GitHub

Visit the project repository: [https://github.com/duydinhlab/DMFAgents](https://github.com/duydinhlab/DMFAgents)


## üß† Citation

Please cite our work if you find this useful:

```bibtex
@article{nguyen2025dmfas,
  title={DropMicroFluidAgents: Autonomous Droplet Microfluidic Research Framework Through Large Language Model Agents},
  author={Nguyen, Dinh-Nguyen and Tong, Raymond Kai-Yu and Dinh, Ngoc-Duy},
  journal={arXiv.2501.14772},
  year={2025},
  note={https://doi.org/10.48550/arXiv.2501.14772}
}



